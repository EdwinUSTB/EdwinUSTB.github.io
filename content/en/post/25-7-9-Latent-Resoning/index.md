---
title: Laboratory Collaborates with ByteDance, Fudan University, Peking University and Others to Release "A Survey on Latent Reasoning"
date: 2025-07-09
draft: false

# Featured image
image:
  focal_point: ""
  preview_only: false
  width: 1000
  height: 500

authors:
  - Jiaheng lIU

tags:
  - Research Achievements
  - Survey

categories:
  - News
  - Research Achievements

summary: Our research team has released an important survey on latent reasoning in collaboration with leading institutions.

---

Large language models (LLMs) have demonstrated impressive reasoning capabilities, especially when guided by explicit chain-of-thought (CoT) reasoning that verbalizes intermediate steps. While CoT improves interpretability and accuracy, its dependence on natural language reasoning limits the model's expressive bandwidth. Latent Reasoning addresses this bottleneck by performing multi-step reasoning entirely within the model's continuous hidden states, eliminating token-level supervision. To advance latent reasoning research, this survey provides a comprehensive overview of the emerging field of latent reasoning. We first examine the foundational role of neural network layers as the computational basis for reasoning, highlighting how hierarchical representations support complex transformations. Next, we explore different latent reasoning approaches, including activation-based recursion, hidden state propagation, and fine-tuning strategies for compressing or internalizing explicit reasoning traces. Finally, we discuss advanced paradigms such as infinite-depth latent reasoning through masked diffusion models, enabling globally consistent and reversible reasoning processes. By unifying these perspectives, we aim to clarify the conceptual landscape of latent reasoning and point toward future directions for research at the cognitive frontiers of LLM.