---
title: 实验室与字节跳动、复旦大学、北京大学等联合发布 A Survey onLatent Reasoning
date: 2025-07-09
draft: false

# Featured image
image:
  caption: 'ICML 2024 会议现场'
  focal_point: ""
  preview_only: false

authors:
  - zhang-professor
  - li-ming

tags:
  - 研究成果
  - 图神经网络
  - ICML

categories:
  - 新闻
  - 研究成果

summary: 我们的研究团队在机器学习顶级会议ICML 2024上发表了关于图神经网络的重要研究成果。

---

大型语言模型 （LLM） 已经展示了令人印象深刻的推理能力，尤其是在由明确的思维链 （CoT） 推理指导时，该推理将中间步骤用语言化。虽然 CoT 提高了可解释性和准确性，但它对自然语言推理的依赖限制了模型的表达带宽。Latent Reasoning 通过完全在模型的连续隐藏状态下执行多步骤推理来解决这一瓶颈，消除了令牌级监督。为了推进潜在推理研究，本调查全面概述了新兴的潜在推理领域。我们首先研究了神经网络层作为推理计算基础的基础作用，重点介绍了分层表示如何支持复杂的转换。接下来，我们探讨了不同的潜在推理方法，包括 基于激活的递归 、 隐藏状态传播和微调策略，用于压缩或内部化显式推理跟踪。最后，我们讨论了高级范式，例如通过掩蔽扩散模型进行无限深度潜在推理 ，从而实现全局一致和可逆的推理过程。通过统一这些观点，我们的目标是阐明潜在推理的概念景观，并为法学硕士认知前沿的研究指明未来的方向。

## 研究背景

知识图谱作为人工智能的重要基础设施，在搜索引擎、推荐系统、问答系统等领域发挥着重要作用。然而，现有的知识图谱往往存在不完整性问题，需要通过推理来补全缺失的事实。

## 主要贡献

本研究提出了一种新型的图神经网络架构，主要创新点包括：

1. **多层次注意力机制**：设计了实体级和关系级的双重注意力机制，能够更好地捕捉知识图谱中的复杂语义关系。

2. **自适应图卷积**：提出了自适应图卷积操作，能够根据不同的关系类型动态调整卷积核参数。

3. **对抗式训练框架**：引入对抗式训练提高模型的泛化能力和鲁棒性。

## 实验结果

在多个标准数据集上的实验结果表明，我们的方法在知识图谱补全任务上显著优于现有方法：

- 在FB15k-237数据集上，Hit@10指标提升了8.5%
- 在WN18RR数据集上，MRR指标提升了12.3%
- 在YAGO3-10数据集上，Hit@1指标提升了15.2%

## 未来展望

这项研究为知识图谱推理提供了新的思路和方法，未来我们将继续在以下方向深入研究：

1. 大规模知识图谱的实时推理
2. 多模态知识图谱的构建与推理
3. 知识图谱在垂直领域的应用

## 团队成员

本研究由张教授指导，博士生李明为第一作者，实验室其他成员也参与了相关工作。

感谢所有支持我们研究工作的同事和合作伙伴！我们将继续努力，在人工智能领域做出更多贡献。
